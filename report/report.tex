\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage[margin=1.2in]{geometry}
\begin{document}

\title{Progress report}
\author{Lee Chu Yong Mark}
\maketitle

\begin{abstract}
Placeholder
\end{abstract}

\section{Introduction}
In recent years, vast amounts of information have become available on the Internet. When an event occurs, information about this event is generated from numerous sources. However, events do not occur in isolation. More often than not, a single event can be connected to other related events in the past, present and future (unless of course, the said event is very recent). It may be difficult for human analysts to spot these connections between events.

TBC - placehold

\section{Background}
Many authors have worked on the discovery of knowledge from news articles. We draw on work from two key areas, clustering for topic detection as well as overlapping community detection for creating chains of news articles.

\subsection{Clustering}
Clustering is the process of examining a collection of points and grouping these points into cluster according to a distance measure \cite{mining2012}. We elaborate on two clustering algorithms, hierarchical agglomerative clustering and KMeans. Both these algorithms assume an Euclidean space. Every cluster has a \textit{centroid}, the average of all its points. The \textit{radius} of a cluster is the maximum distance between all points and the centroid. The \textit{diameter} of a cluster is the maximum distance between any two points in the cluster.

\subsubsection{Agglomerative clustering}
In agglomerative clustering, each point starts as its own cluster. Clusters that are close to each other, based on a distance measure, are combined to form a larger cluster. This process of combining clusters continues until certain conditions are met (e.g. \textit{k} clusters are desired and have been found) or it becomes undesirable to combine two clusters together (e.g. the diameter of the best possible merge exceeds a threshold). Agglomerative clustering suffers from a running time of $O(n^2 log n)$, making it unfeasible when \textit{n} is large. This stems from having to compute the distance between each pair of clusters to find the best merge.

\subsubsection{KMeans}
Unlike agglomerative clustering, which merges clusters together, KMeans is a point clustering algorithm. It assumes that there are \textit{k} clusters. To select these initial \textit{k} clusters, \textit{k} points are chosen from the data and set as the centroids of their clusters. These points are chosen in such a way that they are unlikely to be in the same cluster. For each of the remaining points, the centroid nearest to the point is found, the point is added to the cluster of that centroid and the centroid is adjusted to account for the addition of the point. KMeans runs in linear time with respect to the number of items.

\subsubsection{Bisecting KMeans}
Comparions of clustering approaches have found that while agglomerative clustering tends to produce superior clusters, KMeans is far more efficient \cite{ctechniquecomp}. It is possible to combine the best of both these approaches and produce clusters which are as good or better than clusters produced by agglomerative clustering methods. The algorithm is discussed in \cite{ctechniquecomp} but not utilised due to limited time.

\subsubsection{Clustering and topic detection}
Clustering is used to detect topics. Columbia's Newsblaster \cite{newsblaster}, a news tracker and summariser, utilises agglomerative clustering with a groupwise average similarity function to group articles that belong to the same story together. In \cite{tdttanalysis}, an incremental clustering algorithm is proposed for topic detection.


\subsection{Article chaining} \label{chainalgo}
After generating clusters of articles, we need to link the clusters together to form a chain. We adopt the algorithm found in \cite{infocartography}, which draws on the work in overlapping community detection from \cite{overlapcom}. This algorithm captures the property of \textit{coherence} effectively.

\textit{Coherence} is the idea that every item in a chain should share some characteristics. When the items are article clusters, all the clusters should share a common set of words. While it is simpler to measure the similarity between consecutive clusters in a chain, it can give rise to chains that are incoherent. In an incoherent chain, all consecutive clusters are similar (e.g. clusters 1 and 2 are similar, clusters 2 and 3 are similar). However, clusters 1 and 3 may be very different from each other, to the point of not having any content in common. To apply the concept of coherence to the article clusters, we want to find groups of words that belong to the same clusters and clusters that use similar words \cite{infocartography}.

A weighted bipartite graph is constructed, with the clusters as one set of nodes and individual words as the other. An edge exists between an article cluster and a word if and only if an article in the cluster contains the word. The weight of the edge is the number of times the word occurs in the cluster. By adding weights to the edges, it is possible to ensure strong co-occurence between clusters and words by removing edges with weight that is less than 10\% of the maximum edge weight. 

The weights are then discarded and overlapping commmunites detected using BigClam. BigClam uses a block coordinate gradient ascent approach and is very scalable, with each iteration taking near constant time \cite{overlapcom}.


\section{Algorithm}
In this section, we provide a broad overview of our algorithm. Our algorithm has two main steps.
\begin{enumerate}[noitemsep]
\item{Group articles into article clusters - Agglomerative Clustering}
\item{Chain article clusters together - Overlapping Community Detection}
\end{enumerate}
\subsection{Article clustering} \label{articlecluster}
The initial step involves splitting the articles into time steps of seven days. Articles are vectorised using TF-IDF. TF-IDF aids in the removal of uninformative words by reducing the weight of words that are either rare or occur frequently. 

Latent Semantic Indexing (LSI) is performed. LSI is a method for automated indexing and retrival that takes advantage of implicit higher order structure in the association of terms with documents. LSI was chosen over its successor, Latent Dirichlet Allocation (LDA) as it scales better and has greater noise tolerance.

As we examine many articles at each time step, it is common to find several articles covering the same news story. Articles which cover the same (or very similar) news story are grouped using agglomerative clustering. Agglomerative clustering is favoured over KMeans as it results in smaller clusters that more accurately capture a single story. To further reduce the size of the clusters, clusters with more than 10 articles are split into smaller clusters. This has the added benefit of ensuring consistent cluster size, as certain time steps contain significantly more news articles than others.

\subsection{Chaining articles}
In section \ref{articlecluster}, we have generated clusters of articles. To form chains of articles, the algorithm described in section \ref{chainalgo} was applied on the clusters. Every overlapping community that is detected may contain one of more clusters, each of which contains one or more articles. All the articles in each overlapping community are used to form a chain.

\section{Evaluation}
\subsection{Data}
We used the New York Times (NYT) API to load information on 208 259 articles from the U.S. and World sections from 2013 to June 2015, with 103 414 articles coming from the U.S. section and the remaining 104 845 from the World section. We captured the following information from each article: its title, the section which it came from (either U.S. or World), publication date, word count and a short summary. The data was stored in a SQLite database for further processing.

\subsection{Metrics?}
This section is still very open for discussion. The most common metric seems to be user testing on a specific topic.

\subsection{SQL DB}
\begin{itemize}[noitemsep]
\item{NewYT all (article) - name text, section text, date datetime, wordcnt int, summary text, id int }
\item{NewYT clustered (cluster) - content text, n articles int, first article date datetime, last article date datetime, articles id text (json), articles data text (json), id int}
\end{itemize}

\section{Others}
Have not decided where these might go, but the information here could be useful if anyone else decides to examine this area in the future.

\subsection{Alternative article clustering method}
The algorithm in section \ref{chainalgo} is an extension of an algorithm used to cluster articles together. It entails creating a word co-occurence graph for a set of articles. In such a graph, words are nodes and edges are created between the nodes if they occur in the same document. The top 50 tf-idf words were used for each article \cite{infocartography}. Bigclam is used to find overlapping communities in the graph \cite{overlapcom}. This results in clusters of words and it is unclear to us exactly how these word clusters are linked to the original articles. A possible method of doing so would be to calculate the similarity between the word clusters and the top 50 tf-idf words from each article. If the similarity exceeds some threshold, the article is linked to that word cluster.

\subsection{Temporal summarisation}
Temporal summarisation systems aim to monitor information associated with an event over time \cite{trects2014}. Work in this area aims to create systems that can provide relevant updates as a situation progresses and tracks important event-related attributes. When used to examine a single topic, it can yield greater insight into how events unfolded. Unfortunately, it requires that the topic be identified before it can be utilised \cite{temporalsum}.

\section{Progress}

\subsection{Complete}
\begin{itemize}[noitemsep]
\item{Clean raw articles and put into sqlite DB}
\item{Convert cleaned articles to vectors using LSI topic model, articles are processed in batches of 1 week each}
\item{Agglomerative clustering on these vectors}
\item{Expand largest clusters as needed}
\item{Some error occur while performing clustering. Remove articles that cause errors and try again / save the cluster}
\item{Save these clusters as the top articles}
\item{Store the top articles into some DB as well}
\item{Convert top articles into vectors (LSI)}
\item{Process these using some algorithm to make the chains. Something simple will suffice for now, just to get something out to examine. Toposort?}
\end{itemize}

\subsection{To do - short term}
\begin{itemize}[noitemsep]
\item{Writeup on the background}
\end{itemize}

\subsection{To do - long term}
\begin{itemize}[noitemsep]
\item{Come up with a better way to determine whether a saved cluster (top article) is accurate - a possible solution might be supervised learning, come up with a tool to aid in the annotation}
\item{Switch tfidf to use logent weights}
\item{Improvements on the chains of articles?}
\end{itemize}

\section{Reference list}
% Master reference list
\begin{itemize}[noitemsep]
\item Mining massive dataset \cite{mining2012}.
\item Newsblaster \cite{newsblaster}.
\item Contextual bandit \cite{contextual:li2010}.
\item Clustering survey \cite{clusteringsurvey}.
\item Clustering technique comparison \cite{ctechniquecomp}.
\item Online LDA \cite{onlinelda}.
\item TDT, trend analysis with neural network \cite{tdttanalysis}.
\item TREC temporal summarisation track overview. \cite{trects2014}
\item Topic detection and tracking evaluation overview \cite{tdtoverview}
\item Connecting dots between news articles \cite{connectingdots}
\item Trains of thought: generating information maps \cite{informationmaps}
\item Event threading within news topic \cite{eventthreading}
\item Information cartography \cite{infocartography}
\end{itemize}

\newpage

% F1 -> F11 -> F1 -> F1
\bibliography{references}
\bibliographystyle{ieeetr}
\end{document}